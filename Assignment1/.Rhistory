as.Date(cutoff_time, "%H")
as.Date(5, "%H")
as.Date(5, format = "%H")
as.Date(5, format = "%h")
as.Date(5, format = "%p")
make_datetime(hour = 5)
install.packages("lubridate")
library("lubridate")
make_datetime(hour = 5)
k = make_datetime(hour = 5)
minute(k) = 1
k
library("lubridate")
cutoff_time = qnorm(0.99, 4.5, 0.37)
min_time = (cutoff_time - round(cutoff_time))*60
date_cut = make_datetime(hour = round(cutoff_time), min = min_time)
finaltime = as.character(date_cut, format='%H:%M:%S')
finaltiem
finaltime
library("lubridate")
cutoff_time = qnorm(0.99, 16.5, 0.37)
min_time = (cutoff_time - round(cutoff_time))*60
date_cut = make_datetime(hour = round(cutoff_time), min = min_time)
finaltime = as.character(date_cut, format='%H:%M:%S')
finaltime
cutoff_time_tail1 = qnorm(0.025, 16.5, 0.37)
cutoff_time_tail2 = qnorm(0.975, 16.5, 0.37)
min_time_1 = (cutoff_time_tail1 - round(cutoff_time_tail1))*60
min_time_2 = (cutoff_time_tail2 - round(cutoff_time_tail2))*60
date_cut_1 = make_datetime(hour = round(cutoff_time_1), min = min_time_1)
cutoff_time_tail1 = qnorm(0.025, 16.5, 0.37)
cutoff_time_tail2 = qnorm(0.975, 16.5, 0.37)
min_time_1 = (cutoff_time_tail1 - round(cutoff_time_tail1))*60
min_time_2 = (cutoff_time_tail2 - round(cutoff_time_tail2))*60
date_cut_1 = make_datetime(hour = round(cutoff_time_tail1), min = min_time_1)
date_cut_2 = make_datetime(hour = round(cutoff_time_tail2), min = min_time_2)
finaltime_1 = as.character(date_cut_1, format='%H:%M:%S')
finaltime_2 = as.character(date_cut_2, format='%H:%M:%S')
finaltime_1
finaltime_2
df = data.frame(c(0.58,0.12,NULL), c(0.27,0.03,NULL), c(NULL,NULL,1.00))
df
df = data.frame(a = c(0.58,0.12,NULL), b = c(0.27,0.03,NULL), k = c(NULL,NULL,1.00))
df
df = data.frame("Y=0" = c(0.58,0.27,NaN), "Y = 1" = c(0.12,0.03,NaN), "Total" = c(NaN, NaN, 1.00))
df
df = data.frame("Y \= 0" = c(0.58,0.27,NaN), "Y \= 1" = c(0.12,0.03,NaN), "Total" = c(NaN, NaN, 1.00))
df = data.frame("Y \\= 0" = c(0.58,0.27,NaN), "Y \\= 1" = c(0.12,0.03,NaN), "Total" = c(NaN, NaN, 1.00))
rownames(df) = c("X \\= 0", "X \\= 1", "Total")
df
df = data.frame(c(0.58,0.27,NaN),c(0.12,0.03,NaN), "Total" = c(NaN, NaN, 1.00))
rownames(df) = c("X = 0", "X = 1", "Total")
df
df = data.frame(c(0.58,0.27,NaN),c(0.12,0.03,NaN), c(NaN, NaN, 1.00))
rownames(df) = c("X = 0", "X = 1", "Total")
colnames(df) = c("Y = 0", "Y = 1", "Total")
df
df$Total[1:2,]
df$Tota
df$Total
df[,Total]
df[,"Total"]
df[1:2,"Total"]
df$`Y = 0`
df[1:2,"Total"] = df$`Y = 0` + df$`Y = 1`
df[1:2,"Total"] = df$`Y = 0`[1:2,] + df$`Y = 1`{1:2,}
df[1:2,"Total"] = df$`Y = 0`[1:2,] + df$`Y = 1`[1:2,]
df[1:2,"Total"] = df$`Y = 0`[1:2] + df$`Y = 1`[1:2]
df
df = data.frame(c(0.58,0.27,NaN),c(0.12,0.03,NaN), c(NaN, NaN, 1.00))
rownames(df) = c("X = 0", "X = 1", "Total")
colnames(df) = c("Y = 0", "Y = 1", "Total")
df[1:2,"Total"] = df$`Y = 0`[1:2] + df$`Y = 1`[1:2]
df["Total",] = df["X = 0",] + df["X = 1",]
df
df['X = 1',]
sum(df['X = 1',1:2]*1)
df['Total',0]*0 + df['Total',1]*1
df['Total',0]
df['Total',]
df['Total',1]*0 + df['Total',2]*1
df$Total*weight_x
weight_x = c(0,1,0)
df$Total*weight_x
df['X = 0',1]*0/df['X = 0',3] + df['X = 0',2]*1/df['X = 0',3]
df['X = 1',1]*0/df['X = 1',3] + df['X = 1',2]*1/df['X = 1',3]
df$`Y = 1`[1]*0/df$`Y = 1`[3] + df$`Y = 1`[2]*1/df$`Y = 1`[3]
df$Total[1]*0/df$Total[3] + df$Total[2]*1/df$Total[3]
########### Assignment 1#############
########### Problem 1
##Q1
prob_no_conflict = pnorm(5,4.5,0.37)
prob_cut_in = 1 - prob_no_conflict
## prob_cut_in = 0.08829145
## Therefore, the prob that the baseball game will cut into that program is 0.08829145
##Q2
prob_show = pnorm(4.5,4.5,0.37)
## prob_show = 0.5
## Therefore, the prob of the 0.5h show is 0.5
##Q3
prob_long_show = pnorm(4,4.5,0.37)
## prob_long_show = 0.08829145
## Therefore, the prob of the 1h show is 0.08829145
##Q4
library("lubridate")
cutoff_time = qnorm(0.99, 16.5, 0.37)
min_time = (cutoff_time - round(cutoff_time))*60
date_cut = make_datetime(hour = round(cutoff_time), min = min_time)
finaltime = as.character(date_cut, format='%H:%M:%S')
## finaltime = "17:21:00"
## Therefore, the 99% ensured cut-off time is 17:21:00
## Q5
cutoff_time_tail1 = qnorm(0.025, 16.5, 0.37)
cutoff_time_tail2 = qnorm(0.975, 16.5, 0.37)
min_time_1 = (cutoff_time_tail1 - round(cutoff_time_tail1))*60
min_time_2 = (cutoff_time_tail2 - round(cutoff_time_tail2))*60
date_cut_1 = make_datetime(hour = round(cutoff_time_tail1), min = min_time_1)
date_cut_2 = make_datetime(hour = round(cutoff_time_tail2), min = min_time_2)
finaltime_1 = as.character(date_cut_1, format='%H:%M:%S')
finaltime_2 = as.character(date_cut_2, format='%H:%M:%S')
## finaltime_1 =  "15:47:00"
## finaltime_2 =  "17:13:00"
## Therefore, the time slot of 95% insurance is 15:47:00 - 17:13:00
########## Problem2
####### a
df = data.frame(c(0.58,0.27,NaN),c(0.12,0.03,NaN), c(NaN, NaN, 1.00))
rownames(df) = c("X = 0", "X = 1", "Total")
colnames(df) = c("Y = 0", "Y = 1", "Total")
df[1:2,"Total"] = df$`Y = 0`[1:2] + df$`Y = 1`[1:2]
df["Total",] = df["X = 0",] + df["X = 1",]
#######Therefore, the table will be like
######         Y = 0 Y = 1 Total
######  X = 0  0.58  0.12   0.7
######  X = 1  0.27  0.03   0.3
######  Total  0.85  0.15   1.0
####### b
### It belongs to the discrete distribution
### And the distribution is Bernoulli distribution.
###### c
E_Y = df['Total',1]*0 + df['Total',2]*1
## Therefore, the E(Y) is 0.15 and this number means 15% earnings needs to be restated
## no matter that whether IFE serves on the board.
##### d
weight_x = c(0,1,0)
E_X = sum(df$Total*weight_x)
## Therefore, the E(X) is 0.3 and this number means in 30% cases,
## IFE serves on the firm's board.
##### e
E_Y_0 = df['X = 0',1]*0/df['X = 0',3] + df['X = 0',2]*1/df['X = 0',3]
E_Y_1 = df['X = 1',1]*0/df['X = 1',3] + df['X = 1',2]*1/df['X = 1',3]
## Therefore, the E(Y|X=0) is 0.1714286 and the E(Y|X=1) is 0.1
## E(Y|X=0) means that if there is no IFE served on the board, the expectation of restated earning is 0.1714286
## E(Y|X=1) means that if there is IFE served on the board, the expectation of restated earning is 0.1
## And it tells that having IFEs on corporate boards can reduce the possibility of restated earnings from the data of this table.
##### f
### if I know it has to restates its earning
P_X_1 = df$`Y = 1`[2]*1/df$`Y = 1`[3]
### if I don't know it had to restate its earnings
P_X = df$Total[2]*1/df$Total[3]
## Therefore, if I know it has to restate its earning, the prob is 0.2
## If I don't know whether it has to restate its earning, the prob is 0.3
##### g
## They are not independent, because if they are independent, P(X = 1) = P(X = 1|Y = 1)
## However, in question f, we have proved that those two values are not equal
## Therefore, they are not independent.
P_X
P_X_1
P(X >= 50) = pnorm(40, 45.10607, 2.469763, lower.tail = T)
pnorm(40, 45.10607, 2.469763, lower.tail = T)
pnorm(50, 45.10607, 2.469763, lower.tail = T)
pnorm(50, 45.10607, 2.469763, lower.tail = F)
P(X >= 50) = punif(50, 30, 60, lower.tail = T)
punif(50, 30, 60, lower.tail = F)
qnorm?
sd
?qnorm
qnorm(1,lower.tail = F)
qnorm(1,lower.tail = T)
pnorm(1,lower.tail = T)
pnorm(1,lower.tail = F)
pnorm(50, 45.10607, 2.469763, lower.tail = F)
qnorm(-1)
pnorm(-1)
qnorm(0.95)*0.6 + 4.2
qnorm(-2)
pnorm(2)
pnorm(-2)
pnorm(56,60,4)
pnorm(52,60,4)
pnorm(46,40,3)
qnorm(1.28)
pnorm(-1.28)
pnorm(-1.96)
pnorm(-1.28)
pnorm(2.33)
pnorm(2.33, lower.tail = T)
pnorm(2.33, lower.tail = F)
2*pnorm(-2.33)
##########################################################################################################
###                                      Core Statistics Using R                                       ###
###                                   Simple Linear Regression in R                                    ###
##########################################################################################################
############# Simple linear regression in R
######## Clear
rm(list=ls()); # deleting all objects from the memory
gc(); # garbage collection - releasing memory when an object is no longer used
######## Example: House prices
#### Load data
hprices = read.csv(url("http://hanachoi.github.io/datasets/hprices.csv"), header=TRUE, sep=",")
# hprices = read.csv("hprices.csv", header=TRUE, sep=",") # Set appropriate working directory and/or the file path!
#### Describe and visualize data
head(hprices) # first few rows to see how this dataset looks like
summary(hprices) # get some summary stats
hist(hprices$sqrft) # histogram for house sizes
hist(hprices$sqrft, breaks = 20) # more bins
hist(hprices$price) # histogram for house prices
hist(hprices$price, breaks = 20)
plot(hprices$sqrft, hprices$price) # scatter plot: size versus prices
cor(hprices)
#### Estimate regression coefficients (beta0, beta1)
## Method: Using the formulas directly
beta1_hat = cov(hprices$sqrft, hprices$price) / var(hprices$sqrft)
beta0_hat = mean(hprices$price) - beta1_hat*mean(hprices$sqrft)
c(beta0_hat, beta1_hat)
## Method: Using "lm" command
?lm # see help
lm(price~sqrft, data=hprices) # We use the lm (linear model) command to estimate a regression
# If we want more of the output, we need to summarize the contents of the estimated model
summary(lm(price~sqrft, data=hprices)) # summary of the regression output
fit = lm(price~sqrft, data=hprices) # assign regression output to a new variable/list called "fit"
summary(fit) # same as summary above
abline(lm(hprices$price~hprices$sqrft), col="red") # add a red-color regression line to the existing plot (the last plot created above)
#### Calculating Confidence Intervals
confint(fit, level=0.95) # construct confidence intervals (note: R uses t distribution for CIs)
confint.default(fit, level=0.95) # can also get R to use Normal distribution if you prefer
#### Various sums of squares
anova(fit)
anova(fit)
qnorm(0.1)
qnorm(2)
qnorm(0.99)
qnorm(-0.1)
qnorm(0.1)
qnorm(0.9)
pnorm(0.1)
pnorm(1.28)
pnorm(1.96)
install.packages("tmaptools")
require(tmaptools)
require(tmaptools)
placeReverse <-  rev_geocode_OSM(x = 22.2, y = 11.1,as.data.frame = TRUE)[, c("name")]
placeReverse
placeReverse <-  rev_geocode_OSM(x = 33.8131365, y = -117.9197,as.data.frame = TRUE)[, c("name")]
placeReverse <-  rev_geocode_OSM(y = 33.8131365, x = -117.9197,as.data.frame = TRUE)[, c("name")]
placeReverse
place1 <-  "Eiffel Tower, Paris, France"
location1 <-  geocode_OSM(place1,details = TRUE, as.data.frame = TRUE)
location1
## population proportions
pop <- c("A" = .25, "B" = .25, "C" = .25, "D" = .25)
mean(avg_age)
avg_age <- c("A" = 30, "B" = 40, "C" = 60, "D" = 70)
mean(avg_age)
std_dev_age <- 5
## unbalanced sample
samp <- c("A" = .28, "B" = .4, "C" = .12, "D" = .20)
## close to balance sample
## samp <- c("A" = .27, "B" = .26, "C" = .23, "D" = .24)
samp_size <- 1000
## create sample data
group <- c()
for (i in names(samp)){
group <- c(group,rep(i,samp[i]*samp_size))
}
age <- c()
for (i in names(samp)){
age <- c(age,age = c(rnorm(samp[i]*samp_size, avg_age[i], std_dev_age)))
}
samp_data <- data.frame(group = group, age = age)
## calculate means by group
sapply(split(samp_data$age, samp_data$group), mean)
## calculate mean of entire sample
mean(samp_data$age)
prop.table(table(samp_data$group))
## Chi Squared test
chisq.test(table(samp_data$group), p = pop)
#weights, if reweighting
##formula is w=P/M
weights <-  pop/proportions(table(samp_data$group))
weights
#setting weight for each response in sample
weights["A"]
samp_data$weights <-  weights[samp_data$group]
head(samp_data)
tail(samp_data)
## calculate weighted mean of entire sample
mean(samp_data$age * samp_data$weights)
library(smotefamily)
library(scutr)
library(dplyr)
library(ggplot2)
###
#simulated data with 3 categories
## population proportions
samp <- c("A" = .75, "B" = .15, "C" = .1)
avg_PC1 <- c("A" = 3, "B" = 4, "C" = 2)
std_dev_PC1 <- c("A" = .5, "B" = .8, "C" = .2)
avg_PC2 <- c("A" = 4, "B" = 4, "C" = 3)
std_dev_PC2 <- c("A" = .5, "B" = .8, "C" = .3)
samp_size <- 1000
## create sample data
group <- c()
for (i in names(samp)){
group <- c(group,rep(i,samp[i]*samp_size))
}
pc1 <- c()
for (i in names(samp)){
pc1 <- c(pc1,pc1 = c(rnorm(samp[i]*samp_size, avg_PC1[i], std_dev_PC1[i])))
}
pc2 <- c()
for (i in names(samp)){
pc2 <- c(pc2,pc2 = c(rnorm(samp[i]*samp_size, avg_PC2[i], std_dev_PC2[i])))
}
samp_data <- data.frame(group = group, pc1 = pc1, pc2 = pc2)
ggplot(samp_data, aes(x = pc1, y = pc2, color = group)) + geom_point(size = 3) +
ggtitle("Original Data Set with 3 Categories")
##SCUT - a package to easily use SMOTE, rebalances keeping sample size constant
bal_samp <- SCUT(samp_data, "group")
ggplot(bal_samp, aes(x = pc1, y = pc2, color = group)) + geom_point()
##SMOTE - function SMOTE works easily when there are 2 classes
SMOTE_AB <- SMOTE(samp_data[samp_data$group %in% c("A", "B"), c("pc1", "pc2")],
samp_data[samp_data$group %in% c("A", "B"), "group"])
smote_ab <- SMOTE_AB$data
table(smote_ab$class)
ggplot(smote_ab, aes(x = pc1, y = pc2, color = class)) + geom_point()
## multi-class SMOTE - with more than two classes, handle one at a time
samp_data$B <- ifelse(samp_data$group == "B", 1, 0)
samp_data$C <- ifelse(samp_data$group == "C", 1, 0)
SMOTE_B <- SMOTE(samp_data[ , c("pc1", "pc2")], samp_data$B, K = 3, dup_size = 4)
smote_b <- SMOTE_B$data
smote_b <- smote_b %>%
filter(smote_b$class == 1) %>%
mutate(group = "B") %>%
select(pc1, pc2, group)
SMOTE_C <- SMOTE(samp_data[ , c("pc1", "pc2")], samp_data$C, K = 3, dup_size = 6)
smote_c <- SMOTE_C$data
smote_c <- smote_c %>%
filter(smote_c$class == 1) %>%
mutate(group = "C") %>%
select(pc1, pc2, group)
samp_data_a <- samp_data %>%
filter(samp_data$group == "A") %>%
select(pc1, pc2, group)
smote_abc <- rbind(samp_data_a, smote_b, smote_c)
table(smote_abc$group)
ggplot(smote_abc, aes(x = pc1, y = pc2, color = group)) + geom_point(size = 2) + ggtitle("Data with SMOTE Over-sampling")
ggplot(samp_data, aes(x = pc1, y = pc2, color = group)) + geom_point(size = 3) +
ggtitle("Original Data Set with 3 Categories")
rm(list=ls()); # deleting all objects from the memory
gc()
library(smotefamily)
library(scutr)
library(dplyr)
library(ggplot2)
###
#simulated data with 3 categories
## population proportions
samp <- c("A" = .75, "B" = .15, "C" = .1)
avg_PC1 <- c("A" = 3, "B" = 4, "C" = 2)
std_dev_PC1 <- c("A" = .5, "B" = .8, "C" = .2)
avg_PC2 <- c("A" = 4, "B" = 4, "C" = 3)
std_dev_PC2 <- c("A" = .5, "B" = .8, "C" = .3)
samp_size <- 1000
## create sample data
group <- c()
for (i in names(samp)){
group <- c(group,rep(i,samp[i]*samp_size))
}
pc1 <- c()
for (i in names(samp)){
pc1 <- c(pc1,pc1 = c(rnorm(samp[i]*samp_size, avg_PC1[i], std_dev_PC1[i])))
}
pc2 <- c()
for (i in names(samp)){
pc2 <- c(pc2,pc2 = c(rnorm(samp[i]*samp_size, avg_PC2[i], std_dev_PC2[i])))
}
samp_data <- data.frame(group = group, pc1 = pc1, pc2 = pc2)
ggplot(samp_data, aes(x = pc1, y = pc2, color = group)) + geom_point(size = 3) +
ggtitle("Original Data Set with 3 Categories")
##SCUT - a package to easily use SMOTE, rebalances keeping sample size constant
bal_samp <- SCUT(samp_data, "group")
ggplot(bal_samp, aes(x = pc1, y = pc2, color = group)) + geom_point()
##SMOTE - function SMOTE works easily when there are 2 classes
SMOTE_AB <- SMOTE(samp_data[samp_data$group %in% c("A", "B"), c("pc1", "pc2")],
samp_data[samp_data$group %in% c("A", "B"), "group"])
smote_ab <- SMOTE_AB$data
SMOTE_AB
ggplot(smote_ab, aes(x = pc1, y = pc2, color = class)) + geom_point()
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(gtools)
library(reshape2)
library(foreign)
natData <- read.spss(file.choose(),to.data.frame=TRUE,use.value.labels=FALSE)
natLabData <- read.spss(file.choose(),to.data.frame=TRUE,use.value.labels=TRUE,trim_values=TRUE)
## column names of data file
names(natData)
head(natLabData, 5)
summary(natLabData)
attr(natData, "variable.labels")
for(j in 1:length(natLabData)){
x =natLabData[,j]
if(is.numeric(x)){
hist(x,main=names(natLabData)[j])
} else {
plot(x,main=names(natLabData)[j])
}
}
```{r}
colSums(is.na(natData))
cor <-  cor(natData,use="pairwise.complete.obs")
which(cor > .95 , arr.ind = TRUE)
sum(is.na(cor))
cor[ ,colSums(is.na(cor)) > 0]
popSEX <-  c(.54,.46); #true population values, which we can get from customer system data
sampSEX <-  table(natData$sex)
cbind(popSEX,sampSEX = prop.table(sampSEX))
chisq.test(sampSEX,p=popSEX)
##check length of use
popLU <-  c(.08,.09,.18,.65); #true population values, which we can get from customer system data
sampLU <-  table(natData$use)
cbind(popLU,sampLU = prop.table(sampLU)); #creating table as matrix to print it
chisq.test(sampLU,p = popLU)
#create indexing for performance and importance variables
perfVars <-  c("reliavrg","empavrg","tangavrg","respavrg","assuravg")
impVars = c("relimp","empimp","tanimp","resimp","asrimp")
#create full names for performance and importance variables
genVarsf <-  c("Reliability","Empathy", "Tangibles","Responsiveness","Assurance")
perfVarsf <-  paste(genVarsf,"Perf.")
impVarsf <-  paste(genVarsf,"Imp.")
#calculate means and standard errors for performance
perfMeans <-  colMeans(natData[,perfVars],na.rm=TRUE)
perfSE <-  apply(natData[,perfVars],2,sd,na.rm=TRUE)/ sqrt(colSums(!is.na(natData[,perfVars])))
#calculate Means and standard errors for importance
impMeans <-  colMeans(natData[,impVars],na.rm=TRUE)
impSE <-  apply(natData[,impVars],2,sd,na.rm=TRUE)/sqrt(colSums(!is.na(natData[,impVars])))
perfImpDF <-  data.frame(genVarsf,perfVars,perfVarsf,perfMeans,perfSE,impVars,impVarsf,impMeans,impSE)
perfImpDF
#create error bar plots for the performance variables and the importance variables
#first barplot with standard errors for the performance variables in descending order of means
ggplot(perfImpDF,aes(x = reorder(genVarsf,-perfMeans), y=perfMeans, ymax = perfMeans + perfSE, ymin = perfMeans - perfSE)) +
geom_bar(position = position_dodge(width =.75),stat ="identity",col=1,fill=2,width=.75) +
geom_errorbar(position = position_dodge(width=.75), width=1) + labs(x = "Performance", y ="Mean Rating (1-7)")
##And creating similar graph for importance.
ggplot(perfImpDF,aes(x = reorder(genVarsf,-impMeans), y = impMeans, ymax = impMeans + impSE, ymin = impMeans - impSE)) +
geom_bar(position = position_dodge(width=.75), stat = "identity", col=1,fill=2,width=.75) +
geom_errorbar(position = position_dodge(width=.75), width=1) + labs(x ="Importance",y ="Mean of Allocated Out of 100")
combos <-  combinations(5,2)
combos <-  data.frame(combos,v1=perfVarsf[combos[,1]],v2=perfVarsf[combos[,2]],means=numeric(nrow(combos)),p.values=numeric(nrow(combos)))
for(i in 1:nrow(combos)){
t_tmp <-  t.test(natData[,perfVars[combos[i,1]]],natData[,perfVars[combos[i,2]]],paired=TRUE)
combos[i,c("means","p.values")] <-  c(t_tmp$statistic,t_tmp$p.value)
}
## When many hypotheses are tested at once, there is an increased chance of making a Type 1 error (inappropraitely rejecting the null ##hypothesis). Another way to look at it, is that the p-values may be too low, when performing many tests at once.
## There are multiple methods used to correct this issue. One of the simplest, early methods was Bonferroni's simple method, which simply mutiplies the p-value by the total number of hypotheses tested. In our example above, this is 12.
##Other methods provide stronger tests, which are illustrated below.
##Construct adjusted p-values with Bonferroni's simple method
##bf.p.values =  p-value*M, where M is number of hypothesis tests
combos <-  data.frame(combos,bf.p.values = combos[,"p.values"]*nrow(combos))
##Adjust p-values with a more powerful method. For additional information check help on p.adjust
combos <-  data.frame(combos,adj.p.values=p.adjust(combos[,"p.values"],method="hommel"))
combos
perfMeansByProb <-  apply(natData[,perfVars],2,tapply,natData$prob,mean,na.rm=TRUE)
colnames(perfMeansByProb) <-  genVarsf
perfMeansByProb <-  data.frame(prob = c("Problem","No Problem"),
perfMeansByProb)
perfMeansByProbLong <-  melt(perfMeansByProb, id=c("prob"))
ggplot(perfMeansByProbLong, aes(x = variable, y = value, fill = prob))+
geom_bar(position = position_dodge(),stat = "identity",col = 1) +
labs(x = "Performance", y = "Mean Rating (1-7)")
probTests <- matrix(nrow=length(perfVars),ncol=4);
rownames(probTests) <- perfVarsf
for(i in 1:length(perfVars)){ #looping over performance variables
slm <- summary(lm(natData[,perfVars[i]]~prob,data = natLabData)) #runs regression
probTests[i,] = slm[[4]][2,] #saves the row in the summary table corresponding to the variable
}
colnames(probTests) <-  colnames(slm[[4]])
probTests
########### Part 1
############# a)
setwd("/Users/fanfan/Documents/2022fallB/GBA436R/Assignment1")
dt = read.csv("Homework 1 Data - 436R.csv", header=TRUE, sep=",")
############# b)
head(dt, 10)
############# c)
treatmentDate = dt[dt$isTreatmentPeriod == 1, ]$date
earliestDate = sort(treatmentDate, decreasing = F)[1]
earliestDate
############# d)
treatmentGroup = dt[dt$isTreatmentGroup == 1, ]
fitTreat = lm(log(revenue)~isTreatmentPeriod, data = treatmentGroup)
summary(fitTreat)
############# e)
controlGroup = dt[dt$isTreatmentPeriod == 0, ]
fitControl = lm(log(revenue)~isTreatmentGroup, data = controlGroup)
summary(fitControl)
treatmentPeriod = dt[dt$isTreatmentPeriod == 1, ]
fitDMA = lm(log(revenue)~isTreatmentGroup, data = treatmentPeriod)
summary(fitDMA)
treatmentPeriod = dt[dt$isTreatmentPeriod == 1, ]
fitDMA = lm(log(revenue)~isTreatmentGroup, data = treatmentPeriod)
summary(fitDMA)
